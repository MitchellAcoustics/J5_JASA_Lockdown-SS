---
title: "2021 Version of the Multi-level Lockdown Model"
author: "Andrew Mitchell"
date: "05/05/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
verbosity = 1
```

## Reporting

We will be using the TRIPOD (Transparent reporting of multivariable prediction model for individual prognosis or diagnosis) framework for reporting the model development, validation, and results.

## Outcome

### 6a Clearly define the outcome that is predicted by the prediction model, including how and when assessed.

This study involves predicting five outcome variables, through five separately constructed models. These outcomes are survey responses as collected according to the SSID protocol through in-situ questionnaires in various urban public spaces. The five outcome variables are:

1.  `Pleasant` - from (-1, 1) derived from the 8 PAQs according to ISO 12913 Part 3
2.  `Eventful` - from (-1, 1) derived from the 8 PAQs according to ISO 12913 Part 3
3.  `Human` - from (1, 5) survey response regarding the relative presence/dominance of human sounds
4.  `Traffic` - from (1, 5) survey response regarding the relative presence/dominance of traffic sounds
5.  `Natural` - from (1, 5) survey response regarding the relative presence/dominance of natural sounds

### 6b Report any actions to blind assessment of the outcome to be predicted

N/A

## Predictors

### 7a Clearly define all predictors used in developing or validating the multivariable prediction model, including how and when they were measured.

The acoustic predictors are derived from the 30s binaural recordings taken during the SSID Protocol. The (psycho)acoustic metrics considered are:

-   Zwicker Psychoacoustic Annoyance ($PA$)

-   Loudness ($N_5$, sones)

-   Sharpness (acum)

-   Roughness (asper)

-   Impulsiveness (iu)

-   Fluctuation Strength (vacil)

-   Tonality (tuHMS)

-   $L_{Aeq,30s}$ (dB)

-   $L_{A10} - L_{A90}$ (dB)

-   $L_{Ceq} - L_{Aeq}$ (dB)

-   ~~Speech Interference Level ($SIL3_{50}$, dB)~~

The (psycho)acoustic predictors investigated were selected in order to describe many aspects of the recorded sound - in particular, the goal was to move beyond a focus on sound level. Three predictors (Loudness, LAeq, and SIL3) capture various aspects of the level of the sound, with LAeq representing the most common and traditional environmental acoustic metric, Loudness providing a more complex metric of the perception of the sound level, and SIL3 representing to what degree the ambient sound interferes wit casual conversation in the public space. Sharpness and LCeq-LAeq each describe aspects of the spectral content of the sound, with Sharpness describing the degree of high-frequency content and LCeq-LAeq capturing the degree of low-frequency content. LA10-LA90 attempts to describe the variability of the sound level over the recording period, essentially representing the level difference between the foreground sound and the background sound. Psychoacoustic Annoyance is calculated according to Zwicker (1999), as an empirically derived function of Loudness ($N_5$), Sharpness, Fluctuation Strength, and Roughness. 

Each of these predictors was calculated from the 30s binaural recordings with Artemis. The maximum value from the left and right channels of the binaural recording are used, as suggested in ISO 12913-3. 

```{r predictors, warning=FALSE, message=FALSE}
init_features <- c("PA(Zwicker)", "Loudness_N5(soneGF)", "Sharpness_S(acum)", "Rough_HM_R(asper)", "I_HM_I(iu)", "FS_(vacil)", "Ton_HM_Avg,arith(tuHMS)", "LAeq_L(A)(dB(SPL))", "LA10_LA90(dB(SPL))", "LCeq_LAeq(dB(SPL))")
init_feature_string <- paste(init_features, collapse = " + ")
init_feature_string
```

### 7b Report any actions to blind assessment of predictors for the outcome and other predictors.

N/A

## Sample Size

### 8 Explain how the study size was arrived at.
(Covered in the manuscript already)

```{r load data, message=FALSE, warning=FALSE}
library(glue)
library(tidyr)
library(here)
library(knitr)
library(dplyr)

library(readxl)
data <- read_excel(here("data", "2021-05-18", "SSID Europe Database V1.0.xlsx"), sheet = "Master Merge")
data <- as_tibble(data)
data <- data[c("GroupID", "LocationID", "Lockdown", "ISOPleasant", "ISOEventful", "Human", "Traffic", "Natural", init_features)]

data <- rename(data, "PA" = "PA(Zwicker)")
data <- rename(data, "N5" = "Loudness_N5(soneGF)")
data <- rename(data, "S" = "Sharpness_S(acum)")
data <- rename(data, "R" = "Rough_HM_R(asper)")
data <- rename(data, "I" = "I_HM_I(iu)")
data <- rename(data, "FS" = "FS_(vacil)")
data <- rename(data, "T" = "Ton_HM_Avg,arith(tuHMS)")
data <- rename(data, "LAeq" = "LAeq_L(A)(dB(SPL))")
data <- rename(data, "LA10_LA90" = "LA10_LA90(dB(SPL))")
data <- rename(data, "LC_LA" = "LCeq_LAeq(dB(SPL))")
# data <- rename(data, "SIL3" = "SIL3_Avg,arith(dB(SPL))")

features <- c("PA", "N5", "S", "R", "I", "FS", "T", "LAeq", "LA10_LA90", "LC_LA", "Human", "Traffic", "Natural")
feature_string <- paste(features, collapse = " + ")
feature_string

locations <- c("CamdenTown", "EustonTap", "MarchmontGarden", "MonumentoGaribaldi", "PancrasLock", "RegentsParkFields", "RegentsParkJapan", "RussellSq", "SanMarco", "StPaulsCross", "StPaulsRow", "TateModern", "TorringtonSq")

data <- data %>% 
    filter(LocationID %in% locations)

print(glue("Full data table has dimensions: ", dim(data)[1], ", ", dim(data)[2]))
kable(head(data))
```

## Missing Data

### 9 Describe how missing data were handled (e.g. complete-case analysis, single imputation, multiple imputation) with details of any imputation method

#### Dealing with PAQ missing data

According to our data quality rules, any survey responses with more than half of the PAQ responses missing were excluded whole-sale. Otherwise, where PAQ responses are missing, they are given a value of 3 (Neutral) prior to the ISO trigonometric projection to the complex Pleasantness and Eventfulness values.

#### Missing Sound Source data

Survey responses missing a response for the target outcome are excluded only from that model. e.g. a response may be missing only the Natural sound question - it is excluded only from the Natural model and will be included in all other models (e.g. Traffic, Human, Pleasant,)

#### Missing acoustic data

Only responses which have a corresponding recording are included in this dataset, therefore there is no missing acoustic data for a given survey response.

## Statistical Analysis Methods

### 10a Describe how predictors were handled in the analyses

All predictors are numeric values, in the units described earlier. Before conducting feature selection, the input features are z-scaled to ensure proper comparison of their effect sizes. After the feature selection, when reporting the final fitted models, both scaled and unscaled coefficients will be reported.

```{r treat features, warning=FALSE, message=FALSE}
data <- data %>% mutate_at(vars(GroupID, LocationID, Lockdown), funs(as.factor))
# data <- data %>% mutate_at(vars(Traffic, Human, Natural), funs(factor), order=TRUE)
data <- data %>% mutate_at(vars(c("ISOPleasant", "ISOEventful", features)), funs(as.numeric))

kable(head(data))
```

#### Internal predictors correlations

```{r correlations, warning=FALSE}
library(correlation)

corrs <- correlation::correlation(data[c(features, "LocationID")], multilevel=FALSE)
s <- summary(corrs)
kable(s)
```

### 10b Specify type of model, all model-building procedures (including any predictor selection), and method for internal validation.

#### Multilevel Models

We use two types of multilevel models for this analysis, according to the type of outcome variable. For ISOPleasant and ISOEventful, which are essentially continuous numeric variables, we use a multilevel linear regression. 

#### Train-test split

We use an 80/20 train/test split in order to evaluate the over- / under-fitting of the model. The splits are balanced within each LocationID so that we are randomly sampling from each location equally.

```{r, warning=FALSE, message=FALSE}
prelockdownData <- data %>% filter(Lockdown == 0)
lockdownData <- data %>% filter(Lockdown == 1)
lockdownData <- lockdownData %>% filter(!if_any(c(PA, N5, S, R, I, FS, T, LAeq, LA10_LA90, LC_LA), is.na))
prelockdownData <- prelockdownData %>% filter(!if_any(c(ISOPleasant, ISOEventful, PA, N5, S, R, I, FS, T, LAeq, LA10_LA90, LC_LA, Natural, Traffic, Human), is.na))


# Train-test split
library(groupdata2)
set.seed(42)

parts = partition(prelockdownData, p=0.2, cat_col="LocationID")

train = parts[[2]]
test = parts[[1]]
kable(summary(train))
```

```{r, warning=FALSE, message=FALSE}
kable(summary(test))
```

#### Feature Selection

We use a forward-step feature selection process, where features are added one-by-one to the model and the best performing features are retained until no performance increase is observed. 

```{r, message=FALSE, warning=FALSE, echo=FALSE}
model.building <- function(target, features, train, test, grp="LocationID", exclude.features = NULL, scale=TRUE, pred = TRUE, plot = TRUE, print.loc.means = TRUE, verbose = 0, ...) {
    if (!require("pacman")) install.packages("pacman")
    pacman::p_load(lmerTest, glue, knitr, car, sjPlot, MuMIn, ggplot2, gridExtra, ModelMetrics)
    # Remove the excluded features from the feature vector
    if (length(exclude.features)>=1) {
        features <- grep(paste0(exclude.features, collapse = "|"), features, invert=TRUE, value=TRUE)
    }
    features.string <- paste(features, collapse = " + ")
    formula <- as.formula(glue(target, " ~ 1 + ", features.string, " + (1 + ", features.string, "|", grp, ")"))
    
    train.data <- train
    test.data <- test
    if (scale) {
        train.data[c(features)] <- scale(train.data[c(features)])
    }
    
    # Fit initial model and run stepwise feature selection
    if (verbose > 0) {print("Fitting initial model.")}
    init.model <- lmer(formula, data=train.data)
    if (verbose >= 2) {print(summary(init.model))}
    
    if (verbose > 0) {print("Performing Feature Selection")}
    step.model <- step(init.model, reduce.random=TRUE, alpha.random=0.1, alpha.fixed=0.05)
    if (verbose >= 2) {print(summary(step.model))}
    
    scaled.model <- get_model(step.model)
    formula <- formula(scaled.model)

    if (verbose > 0) {print("Fitting Final Model")}
    model <- lmer(formula, data=train)
    
    if (verbose > 1) {
        print(step.model$fixed)
        print(step.model$random)
        print(summary(model))
    }

    # kable(vif(model))
    
    results <- list("step.results" = step.model, "model" = model, "scaled.model"=scaled.model)

    
    if (plot | pred) {
        # print(tab_model(model, show.icc=TRUE, collapse.ci=TRUE, show.aic=TRUE))
        print(plot_model(scaled.model, type="re", title=glue("Random effects - ", target), show.values=TRUE))
        print(plot_model(scaled.model, show.values=TRUE))
        
        print("Trained model MLM R-squared:")
        print(r.squaredGLMM(model))
        
        # Generating predictions and evaluating for training and test set
        pred.results <- pred.plots(model, target, train.data, test.data, grp=grp, plot=plot)
        
        results[["pred.results"]] = pred.results
        
        means <- location.score(results, print=print.loc.means)
        results[["train.means"]] = means$train.means
        results[["test.means"]] = means$test.means
        results[["train.loc.mae"]] = means$train.loc.mae
        results[["train.loc.r2"]] = means$train.loc.r2
        results[["test.loc.mae"]] = means$test.loc.mae
        results[["test.loc.r2"]] = means$test.loc.r2
        
    }
    
    results
}

pred.plots <- function(model, target, train, test, grp="LocationID", plot) {
    if (!require("pacman")) install.packages("pacman")
    pacman::p_load(knitr, ModelMetrics, gridExtra, ggplot2, glue)
    
    train.pred <- train[grp]
    train.pred$actual <- train[target][[1]]
    train.pred$pred <- predict(model)
    
    test.pred <- test[grp]
    test.pred$actual <- test[target][[1]]
    test.pred$pred <- predict(model, newdata=test)
    
    # Calculate the normalised RMSE
    # range <- abs(max(train.pred$actual) - min(train.pred$actual))
    train.mae <- mae(train.pred$pred, train.pred$actual)
    test.mae <- mae(test.pred$pred, test.pred$actual)
    
    if (plot) {
        # Plot predicted vs actual
        test.title <- glue("Test Set (MAE=", round(test.mae, 3), ")", sep="")
        test.plot <- ggplot(test.pred, aes(x=pred, y=actual, colour=LocationID)) +
            geom_point() +
            geom_abline(intercept=0, slope=1) + 
            labs(x = 'Predicted Values', y='Actual Values', title=test.title)+
            coord_fixed() +
            theme(legend.position="none")
        
        train.title <- glue("Train Set (MAE=", round(train.mae, 3), ")", sep="")
        train.plot <- ggplot(train.pred, aes(x=pred, y=actual, colour=LocationID)) +
            geom_point() +
            geom_abline(intercept=0, slope=1) + 
            labs(x = 'Predicted Values', y='Actual Values', title=train.title) +
            coord_fixed() +
            theme(legend.position="none")
        grid.arrange(train.plot, test.plot, nrow=1)
    }
    
    results <- list("train.pred" = train.pred, "test.pred" = test.pred, "test.mae" = test.mae, "train.mae" = train.mae)
}

location.score <- function(model.results, print=TRUE) {
    pacman::p_load(dplyr, ModelMetrics, MLmetrics, glue)

    pred.results <- model.results$pred.results
    train.pred <- pred.results$train.pred
    test.pred <- pred.results$test.pred
    model <- model.results$model
    
    train.means <- train.pred %>% 
        group_by(LocationID) %>%
        summarise_at(vars(actual, pred), funs(mean(., na.rm=TRUE)))
    test.means <- test.pred %>%
        group_by(LocationID) %>%
        summarise_at(vars(actual, pred), funs(mean(., na.rm=TRUE)))
    
    train.loc.mae <- mae(train.means$pred, train.means$actual)
    train.loc.r2 <- R2_Score(train.means$pred, train.means$actual)

    test.loc.mae <- mae(test.means$pred, test.means$actual)
    test.loc.r2 <- R2_Score(test.means$pred, test.means$actual)
    
    if (print) {
        print(glue("Train Location R^2: ", round(train.loc.r2, 3)))
        print(glue("Test Location R^2:  ", round(test.loc.r2, 3)))
    }
    
    results <- list(
        "train.means" = train.means,
        "test.means" = test.means,
        "train.loc.mae" = train.loc.mae, 
        "train.loc.r2" = train.loc.r2, 
        "test.loc.mae" = test.loc.mae, 
        "test.loc.r2" = test.loc.r2)
}

```


## Pleasant Model
When this feature selection is performed with the entire feature set, it includes both LAeq and SIL3 at the end, which have a VIF > 10. Therefore, we remove one of them (LAeq) at the start before the feature selection is performed. 

### Scores

To score the performance of the training and testing sets, we use the Mean Absolute Error (MAE). This calculates the average distance between the predicted and actual responses of each individual in the train and test sets, telling us, on average, how close our prediction is. The MAE is in the scale of the response feature - for ISOPleasant this means our response can range from -1 to +1 and we are on average off by 0.258. The goal is to 1) get the MAE as low as possible, and 2) for the test and train MAE to be as close as possible. This indicates that we are not over- or under-fitting the model.

However, since we are actually interested in the behaviour of the Location overall and not in the individual responses, we would also like to score the performance of the model in predicting the average response in the location. To do this, we first calculate the mean actual and predicted response for the location, then calculate the $R^2$ value for predicting across all locations. This is where our model really shines, demonstrating a high $R^2$ for predicting the location means.


```{r, message=FALSE, warning=FALSE, cache=TRUE}
Pleasant.results <- model.building(target="ISOPleasant", features = c(features), train = train, test = test, grp="LocationID", verbose=verbosity, pred=TRUE, plot=TRUE)
```

## Eventful Model

```{r, message=FALSE, warning=FALSE, cache=TRUE}
Eventful.results <- model.building(target="ISOEventful", features = features, train = train, test = test, grp="LocationID", verbose=1, pred=TRUE, plot=TRUE)
```

```{r}
sjPlot::tab_model(Pleasant.results$model, Eventful.results$model)
sjPlot::tab_model(Pleasant.results$scaled.model, Eventful.results$scaled.model)
sjPlot::plot_models(Pleasant.results$scaled.model, Eventful.results$scaled.model, std.est='std', show.values = TRUE)

```

## Lockdown Predictions

```{r, warning=FALSE}
# lockdownData$ISOPleasant <- predict(Pleasant.results$model, newdata=lockdownData)
# lockdownData$ISOEventful <- predict(Eventful.results$model, newdata=lockdownData)
# lockdownData$Natural <- predict(Natural.mod, newdata=lockdownData)
# lockdownData$Human <- predict(Human.mod, newdata=lockdownData)
# lockdownData$Traffic <- predict(Traffic.mod, newdata=lockdownData)
# print(head(lockdownData))
# write.csv(lockdownData, glue("Predicted Lockdown Data_", format(Sys.Date()), "_3", ".csv"), row.names = FALSE)
```

