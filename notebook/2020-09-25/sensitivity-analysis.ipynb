{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.5 64-bit ('AcousticAnalysis_2': conda)",
   "display_name": "Python 3.7.5 64-bit ('AcousticAnalysis_2': conda)",
   "metadata": {
    "interpreter": {
     "hash": "6b85c301c15123bd4538818f5210e8d7c60f940ee06dd4af10f01e9aa7d8a703"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Sensitivity Analysis for the Lockdown multilevel model\n",
    "\n",
    "Through the feature selection process, we have created a series of models to describe the soundscape perception. These have ended up with around 5 individual level variables defining the model for each perceptual attribute. What I'd like to find out is which of these parameters contributes the most to the uncertainty of the model. For this stage, we are disregarding continuous variables, so we should be able to run all of this with SALib within python.\n",
    "\n",
    "## Load in the data and models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Initial features number:     119\nprelockdownData shape:      (661, 132)\nduring lockdownData shape:  (573, 132)\n"
    }
   ],
   "source": [
    "import os\n",
    "sys.path.append(\"C:\\\\Users\\\\Andrew\\\\OneDrive - University College London\\\\_PhD\\\\Papers - Drafts\\\\J5_JASA_Lockdown-SS\")\n",
    "\n",
    "from scripts import lockdown_mlm as mlm\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import time\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Define some constants and options\n",
    "## variables\n",
    "dep_vars = [\"Natural\", \"Traffic\", \"Human\", \"Other\", \"loudness\", \"overall\", \"Pleasant\", \"Eventful\"]\n",
    "\n",
    "FEATS_LISTS = mlm.FEATS_LISTS\n",
    "remove = [\"FS_TEMP\", \"LAeq_TEMP\", \"LCeq_TEMP\", \"LZeq_TEMP\", \"I_TEMP\", \"N_TEMP\", \"R_TEMP\", \"S_TEMP\", \"SIL_TEMP\", \"THD_TEMP\", \"T_TEMP\"]\n",
    "\n",
    "for k in remove:\n",
    "    FEATS_LISTS.pop(k, None)\n",
    "\n",
    "acoustic_vars = sorted({x for v in FEATS_LISTS.values() for x in v})\n",
    "\n",
    "## processing options\n",
    "nonlinear_transformations = [] # Leave a blank list to do no transformations\n",
    "criterion = \"aic\"\n",
    "\n",
    "# ##################################################################\n",
    "# Load Data\n",
    "\n",
    "DATA_DIR = Path(\"C:/Users/Andrew/OneDrive - University College London/_PhD/Papers - Drafts/J5_JASA_Lockdown-SS/data\")\n",
    "RESULTS_DIR = Path(\"C:/Users/Andrew/OneDrive - University College London/_PhD/Papers - Drafts/J5_JASA_Lockdown-SS/results\")\n",
    "ssidData = pd.read_csv(DATA_DIR.joinpath(\"2020-08-13/LondonVeniceBINResults_2020-08-13_4.csv\"))\n",
    "\n",
    "for col_name in [\"Lockdown\"]:\n",
    "    ssidData[col_name] = ssidData[col_name].astype('category')\n",
    "\n",
    "# Cutdown the dataset\n",
    "cols = [\"GroupID\", \"LocationID\", \"SessionID\", \"Lockdown\"] + dep_vars + acoustic_vars\n",
    "ssidData = ssidData[cols]\n",
    "\n",
    "# Compress to mean of each GroupID\n",
    "# compressData = ssidData.copy()\n",
    "compressData = ssidData.groupby([\"GroupID\"]).mean()\n",
    "compressData = compressData.merge(ssidData[[\"GroupID\", \"LocationID\", \"SessionID\", \"Lockdown\"]].drop_duplicates(),  on=\"GroupID\")\n",
    "\n",
    "location_codes = pd.Categorical(compressData[\"LocationID\"]).codes\n",
    "compressData[\"LocationID_codes\"] = location_codes\n",
    "compressData.loc[compressData[\"Lockdown\"] == 1].dropna(inplace=True)\n",
    "compressData = compressData.dropna(subset=acoustic_vars)\n",
    "\n",
    "compressData, acoustic_vars = mlm.nonlinear_features(compressData, acoustic_vars, transformations=nonlinear_transformations)\n",
    "print(\"Initial features number:    \", len(acoustic_vars))\n",
    "\n",
    "# Standardise\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "compressData = compressData.replace([np.inf, -np.inf], np.nan)\n",
    "compressData = compressData.dropna(subset=acoustic_vars)\n",
    "scaler = StandardScaler()\n",
    "compressData[acoustic_vars] = scaler.fit_transform(compressData[acoustic_vars])\n",
    "\n",
    "\n",
    "# ###############################################################\n",
    "# Split Prelockdown from during lockdown\n",
    "prelockdownData = compressData.loc[compressData[\"Lockdown\"] == 1]\n",
    "prelockdownData = prelockdownData.dropna()\n",
    "print(\"prelockdownData shape:     \", prelockdownData.shape)\n",
    "lockdownData = compressData.loc[compressData[\"Lockdown\"] == 2]\n",
    "print(\"during lockdownData shape: \", lockdownData.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Pleasant params:\n Intercept               0.271342\nLZeq                   -0.097666\nTHD_95                  0.018068\nPeakSpectralCentroid   -0.038564\nLZeq_10_LZeq_90         0.017302\nI                       0.006066\ndtype: float64\n\nEventful params:\n Intercept    0.122624\nR_50         0.066210\nLAeq_Min     0.041487\ndtype: float64\n"
    }
   ],
   "source": [
    "pleasant_model = sm.load_pickle(str(RESULTS_DIR.joinpath(\"2020-09-01/Pleasant_rirs_model_1.pickle\")))\n",
    "\n",
    "print(\"Pleasant params:\\n\",pleasant_model.fe_params)\n",
    "\n",
    "eventful_model = sm.load_pickle(str(RESULTS_DIR.joinpath(\"2020-09-01/Eventful_rirs_model_1.pickle\")))\n",
    "\n",
    "print(\"\\nEventful params:\\n\",eventful_model.fe_params)"
   ]
  },
  {
   "source": [
    "## Import SALib"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SALib.sample import saltelli\n",
    "from SALib.analyze import sobol"
   ]
  },
  {
   "source": [
    "## Define the testing Model Inputs\n",
    "Next, we must define the model inputs for the SA. Our pleasant model has 5 parameters (`LZeq`, `THD_95`, `PeakSpectralCentroid`, `LZeq_10_LZeq_90`, `I`). For each of these, we need to let the saltelli sampling function know what the range of values is, so we'll pull that from the original dataset. Note, these values will be z-scaled so won't look necessarily correct."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pleasant_problem = {\n",
    "    'num_vars': 5,\n",
    "    'names': ['LZeq', 'THD_95', 'PeakSpectralCentroid', 'LZeq_10_LZeq_90', 'I'],\n",
    "    'bounds': [\n",
    "        [prelockdownData.LZeq.min(), prelockdownData.LZeq.max()],\n",
    "        [prelockdownData.THD_95.min(), prelockdownData.THD_95.max()],\n",
    "        [prelockdownData.PeakSpectralCentroid.min(), prelockdownData.PeakSpectralCentroid.max()],\n",
    "        [prelockdownData.LZeq_10_LZeq_90.min(), prelockdownData.LZeq_10_LZeq_90.max()],\n",
    "        [prelockdownData.I.min(), prelockdownData.I.max()]\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "source": [
    "## Generate Samples\n",
    "Next, we generate the samples. Since we are performing a Sobol sensitivity analysis, we need to generate samples using the Saltelli sampler, as shown below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pleasant_param_values = saltelli.sample(pleasant_problem, 1000)"
   ]
  },
  {
   "source": [
    "## Run Model\n",
    "SALib is not involved in the evaluation of the mathematical or computational model. If the model is written in Python, then generally you will loop over each sample input and evaluate the model:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pleasant_model.predict(pd.DataFrame(pleasant_param_values, columns=pleasant_problem['names'])).values"
   ]
  },
  {
   "source": [
    "## Perform Analysis\n",
    "With the model outputs calculated, we can finally compute the sensitivity indices. In this example, we use `sobol.analyze`, which will compute first, second, and total-order indices. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Si = sobol.analyze(pleasant_problem, Y)"
   ]
  },
  {
   "source": [
    "`Si` is a Python `dict` with the keys `\"S1\", \"S2\", \"ST\", \"S1_conf\", \"S2_conf\"` and `\"ST_conf\"`. The `_conf` keys store the corresponding confidence intervals, typically with a confidence level of 95%. Use the keyword argument `print_to_console = True` to print all indices. Or, we can print the individualv values from `Si` as shown below. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['LZeq', 'THD_95', 'PeakSpectralCentroid', 'LZeq_10_LZeq_90', 'I']\n[0.89729442 0.02616715 0.05962125 0.01426697 0.00906382]\n"
    }
   ],
   "source": [
    "print(pleasant_problem['names'])\n",
    "print(Si['S1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.89654887 0.02717511 0.05956613 0.01438075 0.00929382]\n"
    }
   ],
   "source": [
    "print(Si['ST'])"
   ]
  },
  {
   "source": [
    "## Repeat for Eventful model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['R_50', 'LAeq_Min']\n[0.88638975 0.11611957]\n"
    }
   ],
   "source": [
    "eventful_problem = {\n",
    "    'num_vars': 2,\n",
    "    'names': ['R_50', 'LAeq_Min'],\n",
    "    'bounds': [\n",
    "        [prelockdownData.R_50.min(), prelockdownData.R_50.max()],\n",
    "        [prelockdownData.LAeq_Min.min(), prelockdownData.LAeq_Min.max()],\n",
    "    ]\n",
    "}\n",
    "eventful_param_values = saltelli.sample(eventful_problem, 1000)\n",
    "Y = eventful_model.predict(pd.DataFrame(eventful_param_values, columns=eventful_problem['names'])).values\n",
    "\n",
    "Si = sobol.analyze(eventful_problem, Y)\n",
    "\n",
    "print(eventful_problem['names'])\n",
    "print(Si['S1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[        nan -0.00283246]\n [        nan         nan]]\n"
    }
   ],
   "source": [
    "print(Si['S2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}