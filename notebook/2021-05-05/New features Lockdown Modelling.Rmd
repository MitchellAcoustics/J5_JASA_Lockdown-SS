---
title: "2021 Version of the Multi-level Lockdown Model"
author: "Andrew Mitchell"
date: "05/05/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Reporting

We will be using the TRIPOD (Transparent reporting of multivariable prediction model for individual prognosis or diagnosis) framework for reporting the model development, validation, and results.

## Outcome

### 6a Clearly define the outcome that is predicted by the prediction model, including how and when assessed.

This study involves predicting five outcome variables, through five separately constructed models. These outcomes are survey responses as collected according to the SSID protocol through in-situ questionnaires in various urban public spaces. The five outcome variables are:

1.  `Pleasant` - from (-1, 1) derived from the 8 PAQs according to ISO 12913 Part 3
2.  `Eventful` - from (-1, 1) derived from the 8 PAQs according to ISO 12913 Part 3
3.  `Human` - from (1, 5) survey response regarding the relative presence/dominance of human sounds
4.  `Traffic` - from (1, 5) survey response regarding the relative presence/dominance of traffic sounds
5.  `Natural` - from (1, 5) survey response regarding the relative presence/dominance of natural sounds

### 6b Report any actions to blind assessment of the outcome to be predicted

N/A

## Predictors

### 7a Clearly define all predictors used in developing or validating the multivariable prediction model, including how and when they were measured.

The acoustic predictors are derived from the 30s binaural recordings taken during the SSID Protocol. The (psycho)acoustic metrics considered are:

-   Loudness ($N_5$, sones)

-   Sharpness (acum)

-   Roughness (asper)

-   Impulsiveness (iu)

-   Fluctuation Strength (vacil)

-   $L_{Aeq,30s}$ (dB)

-   $L_{A10} - L_{A90}$ (dB)

-   $L_{Ceq} - L_{Aeq}$ (dB)

-   Speech Interference Level ($SIL3_{50}$, dB) 

```{r predictors, warning=FALSE, message=FALSE}
init_features <- c("Loudness_N5(soneGF)", "Sharpness_S(acum)", "Rough_HM_R(asper)", "I_HM_I(iu)", "FS_(vacil)", "LAeq_L(A)(dB(SPL))", "LA10_LA90(dB(SPL))", "LCeq_LAeq(dB(SPL))", "SIL3_Avg,arith(dB(SPL))")
init_feature_string <- paste(init_features, collapse = " + ")
init_feature_string
```

## Sample Size

### 8 Explain how the study size was arrived at.
(Covered in the manuscript already)

```{r load data, message=FALSE, warning=FALSE}
library(glue)
library(tidyr)
library(here)
library(knitr)
library(dplyr)

library(readxl)
data <- read_excel(here("data", "2021-05-05", "SSID Europe Database V1.0.xlsx"), sheet = "Master Merge")
data <- as_tibble(data)
data <- data[c("GroupID", "LocationID", "Lockdown", "ISOPleasant", "ISOEventful", "Human", "Traffic", "Natural", init_features)]

data <- rename(data, "N5" = "Loudness_N5(soneGF)")
data <- rename(data, "S" = "Sharpness_S(acum)")
data <- rename(data, "R" = "Rough_HM_R(asper)")
data <- rename(data, "I" = "I_HM_I(iu)")
data <- rename(data, "FS" = "FS_(vacil)")
data <- rename(data, "LAeq" = "LAeq_L(A)(dB(SPL))")
data <- rename(data, "LA10_LA90" = "LA10_LA90(dB(SPL))")
data <- rename(data, "LCeq_LAeq" = "LCeq_LAeq(dB(SPL))")
data <- rename(data, "SIL3" = "SIL3_Avg,arith(dB(SPL))")

features <- c("N5", "S", "R", "I", "FS", "LAeq", "LA10_LA90", "LCeq_LAeq", "SIL3")
feature_string <- paste(features, collapse = " + ")
feature_string

locations <- c("CamdenTown", "EustonTap", "MarchmontGarden", "MonumentoGaribaldi", "PancrasLock", "RegentsParkFields", "RegentsParkJapan", "RussellSq", "SanMarco", "StPaulsCross", "StPaulsRow", "TateModern", "TorringtonSq")

data <- data %>% 
    filter(LocationID %in% locations)

print("Full data table has dimensions: ")
print(dim(data))
kable(print(head(data)))
```


## Missing Data

### 9 Describe how missing data were handled (e.g. complete-case analysis, single imputation, multiple imputation) with details of any imputation method

#### Dealing with PAQ missing data

According to our data quality rules, any survey responses with more than half of the PAQ responses missing were excluded whole-sale. Otherwise, where PAQ responses are missing, they are given a value of 3 (Neutral) prior to the ISO trigonometric projection to the complex Pleasantness and Eventfulness values.

#### Missing Sound Source data

Survey responses missing a response for the target outcome are excluded only from that model. e.g. a response may be missing only the Natural sound question - it is excluded only from the Natural model and will be included in all other models (e.g. Traffic, Human, Pleasant,)

#### Missing acoustic data

Only responses which have a corresponding recording are included in this dataset, therefore there is no missing acoustic data for a given survey response.

## Statistical Analysis Methods

### 10a Describe how predictors were handled in the analyses

All predictors are numeric values, in the units described earlier. Before conducting feature selection, the input features are z-scaled to ensure proper comparison of their effect sizes. After the feature selection, when reporting the final fitted models, both scaled and unscaled coefficients will be reported.

```{r treat features, warning=FALSE, message=FALSE}
data <- data %>% mutate_at(vars(GroupID, LocationID, Lockdown), funs(as.factor))
data <- data %>% mutate_at(vars(c("ISOPleasant", "ISOEventful", "Traffic", "Human", "Natural", features)), funs(as.numeric))
kable(head(data))
```

### 10b Specify type of model, all model-building procedures (including any predictor selection), and method for internal validation.

#### Train-test split

We use an 80/20 train/test split. The splits are balanced within each LocationID so that we are randomly sampling from each location equally.

```{r, warning=FALSE, message=FALSE}
prelockdownData <- data %>% filter(Lockdown == 1)
lockdownData <- data %>% filter(Lockdown == 2)

prelockdownData <- drop_na(prelockdownData)
# prelockdownData[c(features)] <- scale(prelockdownData[c(features)])

# Train-test split
library(groupdata2)
set.seed(42)

parts = partition(prelockdownData, p=0.2, cat_col="LocationID")

train = parts[[2]]
test = parts[[1]]
kable(summary(train))
```

```{r, warning=FALSE, message=FALSE}
kable(summary(test))
```

```{r, message=FALSE, warning=FALSE, echo=FALSE}
model.building <- function(target, features, train, test, grp="LocationID", exclude.features = NULL, print.step.summ = FALSE, scale=TRUE, ...) {
    if (!require("pacman")) install.packages("pacman")
    pacman::p_load(lmerTest, glue, knitr, car, sjPlot, MuMIn, ggplot2, gridExtra, ModelMetrics)
    # Remove the excluded features from the feature vector
    if (length(exclude.features)>=1) {
        features <- grep(paste0(exclude.features, collapse = "|"), features, invert=TRUE, value=TRUE)
    }
    features.string <- paste(features, collapse = " + ")
    formula <- as.formula(glue(target, " ~ 1 + ", features.string, " + (1 + ", features.string, "|", grp, ")"))
    
    train.data <- train
    test.data <- test
    if (scale) {
        train.data[c(features)] <- scale(train.data[c(features)])
    }
    
    # Fit initial model and run stepwise feature selection
    init.model <- lmer(formula, data=train.data)
    print(typeof(train.data))
    step.model <- step(init.model, data=train.data, reduce.random=TRUE)
    scaled.model <- get_model(step.model)
    formula <- formula(scaled.model)

    model <- lmer(formula, data=train)
    
    if (print.step.summ) {
        print(step.model$fixed)
        print(step.model$random)
        print(summary(model))
    }

    # kable(vif(model))
    
    # print(tab_model(model, show.icc=TRUE, collapse.ci=TRUE, show.aic=TRUE))
    print(plot_model(model, type="re", title=glue("Random effects - ", target), show.values=TRUE))
    print(plot_model(model, show.values=TRUE))
    
    print(r.squaredGLMM(model))
    
    # Generating predictions and evaluating for training and test set
    pred.results <- pred.plots(model, target, train.data, test.data, grp=grp)
    
    results <- list("step.results" = step.model, "model" = model, "pred.results"= pred.results, "scaled.model"=scaled.model)
}

pred.plots <- function(model, target, train.data, test.data, grp="LocationID") {
    if (!require("pacman")) install.packages("pacman")
    pacman::p_load(knitr, ModelMetrics, gridExtra, ggplot2, glue)
    
    train.pred <- train.data[grp]
    train.pred$actual <- train.data[target][[1]]
    train.pred$pred <- predict(model)
    
    test.pred <- test.data[grp]
    test.pred$actual <- test.data[target][[1]]
    test.pred$pred <- predict(model, newdata=test.data)
    
    # Calculate the normalised RMSE
    # range <- abs(max(train.pred$actual) - min(train.pred$actual))
    train.mae <- mae(train.pred$pred, train.pred$actual)
    test.mae <- mae(test.pred$pred, test.pred$actual)

    # Plot predicted vs actual
    test.title <- glue("Test Set (MAE=", round(test.mae, 3), ")", sep="")
    test.plot <- ggplot(test.pred, aes(x=pred, y=actual, colour=LocationID)) +
        geom_point() +
        geom_abline(intercept=0, slope=1) + 
        labs(x = 'Predicted Values', y='Actual Values', title=test.title)+
        coord_fixed() +
        theme(legend.position="none")
    
    train.title <- glue("Train Set (MAE=", round(train.mae, 3), ")", sep="")
    train.plot <- ggplot(train.pred, aes(x=pred, y=actual, colour=LocationID)) +
        geom_point() +
        geom_abline(intercept=0, slope=1) + 
        labs(x = 'Predicted Values', y='Actual Values', title=train.title) +
        coord_fixed() +
        theme(legend.position="none")
    grid.arrange(train.plot, test.plot, nrow=1)
    
    results <- list("train.pred" = train.pred, "test.pred" = test.pred, "test.mae" = test.mae, "train.mae" = train.mae)
}

```


## Pleasant Model
When this feature selection is performed with the entire feature set, it includes both LAeq and SIL3 at the end, which have a VIF > 10. Therefore, we remove one of them (LAeq) at the start before the feature selection is performed. 

```{r, message=FALSE, warning=FALSE}
Pleasant.results <- model.building(target="ISOPleasant", features = features, train = train, test = test, grp="LocationID", exclude.features = c("LAeq"))
```


## Eventful Model

```{r, message=FALSE, warning=FALSE}
Eventful.results <- model.building(target="ISOEventful", features = features, train = train, test = test, grp="LocationID")
```

```{r}
sjPlot::tab_model(Pleasant.results$model, Eventful.results$model)
sjPlot::tab_model(Pleasant.results$scaled.model, Eventful.results$scaled.model)
sjPlot::plot_models(Pleasant.results$scaled.model, Eventful.results$scaled.model, std.est='std', show.values = TRUE)

```

## Natural Model

```{r, message=FALSE, warning=FALSE}
Natural.results <- model.building(target="Natural", features = features, train = train, test = test, grp="LocationID")
```

## Traffic Model

```{r, message=FALSE, warning=FALSE}
Traffic.results <- model.building(target="Traffic", features = features, train = train, test = test, grp="LocationID", exclude.features = c("SIL3"))
```

## Human Model

```{r, message=FALSE, warning=FALSE}
Human.results <- model.building(target="Human", features = features, train = train, test = test, grp="LocationID")
```

```{r}
sjPlot::tab_model(Natural.results$model, Traffic.results$model, Human.results$model)
sjPlot::tab_model(Natural.results$scaled.model, Traffic.results$scaled.model, Human.results$scaled.model)
sjPlot::plot_models(Natural.results$model, Traffic.results$model, Human.results$model, std.est='std', show.values=TRUE)

```

##### Tried an attempt at Cross-validation -> not successful

### 5-fold CV

```{r}
# set.seed(42)
# 
# Pleasant_formula <- as.formula(glue("ISOPleasant ~ 1 + ", feature_string, " + (1 + ", feature_string, "|LocationID)"))
# # init_Pleasant_model <- lmerTest::lmer(Pleasant_formula, data = train)
# 
# prelockdownData$ID <- c(1:nrow(prelockdownData))
# 
# # confusion_matrices <- list()
# models <- list()
# accuracy <- c()
# 
# for (i in c(1:5)) {
#     # creating 5 random folds from the cancer data set
#     folds <- caret::createFolds(prelockdownData$ISOPleasant, k=5)
#     
#     # Splitting the data into training and testing
#     test <- prelockdownData[prelockdownData$ID %in% folds[[i]], ]
#     train <- prelockdownData[prelockdownData$ID %in% unlist(folds[-i]), ]
#     
#     # feature selection using lmerTest::step
#     init_Pleasant_model <- lmerTest::lmer(Pleasant_formula, data = train)
#     step_model <- lmerTest::step(init_Pleasant_model, data=train, reduce.random=TRUE)
#     step_model <- get_model(step_model)
#     
#     models[[i]] <- step_model
#     accuracy[[i]] <- cAIC4::cAIC(step_model)
#     
# }
```


